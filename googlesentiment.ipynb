{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15809b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 22:31:02.261950: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a9fb1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 -1.0 37427\n",
      "MAXIMUN LENGTH REVIEW----> »TOO BRIGHT!… NIGHT MODE, PLEASE. HOW MANY REQUESTS DO Y'ALL NEED BEFORE YOU IMPLEMENT A DAMN DARK THEME?!!« Also, I can't help feel like lacking lot features make desktop version wonderful... For instance, ★Interface Personalization★. (I mean, call crazy I consider blindingly hideous white bar impossible-to-see-in-sunlight, thin grey font... aesthetically pleasing. It's actually rather ineffective. It's utilitarian depressing horrible.) Please explain personalize bejesus Chrome which, trapped home desktops, inconveniently stationary thus essentially useless (unless plan hella typing)... Yet comes single personal object carried virtually everyday, everyone, everywhere... An object become profoundly integral who/what ARE modern humans. An object, fosters us sense necessity, roughly 94% Americans purportedly \"cannot live without it\".... When we're considering single profoundly imperative aspect modern existence, smartphone, expectation us content half-assed, visually displeasing, minimalist, cookie-cutter interface, slapped existence with, undoubtedly minimal effort.. WHY?.. It's lazy, boring, ugly, (it's actually *not* weird ass place screen, I'll give y'all that..) It's nevertheless distressing. And kinda insulting users pride unique, creative, individuals.. I digress. Back Interface —it's dumb force people method interacting product.— ★It's experience killer forced, along everyone else world, single, inefficient, uncomfortable rut!★.. —We're people, we're distinctly different, us. With different likes dislikes different views preferences want product, want experience feel.— There even \"light\" \"dark\" themes available. Which would simple solution. Or, better yet, Google: allow users express creating themes upload others apply. (similar desktop themes) This literally used, hideous entire phone. EVERYTHING else (that -isn't- Google app) least *offers* dark theme, pete's sake... like difficult. Hell, used navy grey-themed, whatever was.. +Dare I say, probably effort making damn white, would required simply inject \"theme\" option (disappointingly small) \"settings\" menu, users select two bubbles. Light Dark. Plenty users begging simple OPTIONS, —not forced changes, OPTIONAL CHOICES empower USER— quite time, now. Now, always M.O, Google.. company used user-centric, resource friendly apps, utilized simple easy UI felt natural made sense. Then y'all started getting big britches. So, fire douchebag currently charge graphical user interfacing insists using worst possible interface aspects imaginable. Buddy dude steadily disgracing Google poorly designed, unnecessarily bright, so-overly-simple-that-its-somehow-complicated, \"i-wannabe-iPhone-but-I'm-android\".. follies interface.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>sentences_count</th>\n",
       "      <th>characters_count</th>\n",
       "      <th>spaces_count</th>\n",
       "      <th>count_words</th>\n",
       "      <th>duplicates_count</th>\n",
       "      <th>chars_excl_spaces_count</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>whole_numbers_count</th>\n",
       "      <th>...</th>\n",
       "      <th>spelling_quality</th>\n",
       "      <th>spelling_quality_summarised</th>\n",
       "      <th>ease_of_reading_score</th>\n",
       "      <th>ease_of_reading_quality</th>\n",
       "      <th>ease_of_reading_summarised</th>\n",
       "      <th>grammar_check_score</th>\n",
       "      <th>grammar_check</th>\n",
       "      <th>original_Sentiment</th>\n",
       "      <th>original_Sentiment_Polarity</th>\n",
       "      <th>original_Sentiment_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>I like eat delicious food. That's I'm cooking ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Bad</td>\n",
       "      <td>86.20</td>\n",
       "      <td>Easy</td>\n",
       "      <td>Easy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5 issues</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>This help eating healthy exercise regular basis</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Very good</td>\n",
       "      <td>Good</td>\n",
       "      <td>38.99</td>\n",
       "      <td>Difficult</td>\n",
       "      <td>Difficult</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No issues</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.288462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Works great especially going grocery store</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Very good</td>\n",
       "      <td>Good</td>\n",
       "      <td>48.47</td>\n",
       "      <td>Difficult</td>\n",
       "      <td>Difficult</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No issues</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best idea us</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Very good</td>\n",
       "      <td>Good</td>\n",
       "      <td>119.19</td>\n",
       "      <td>Very Easy</td>\n",
       "      <td>Easy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1 issue</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best way</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Very good</td>\n",
       "      <td>Good</td>\n",
       "      <td>120.21</td>\n",
       "      <td>Very Easy</td>\n",
       "      <td>Easy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No issues</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     App                                  Translated_Review  \\\n",
       "0  10 Best Foods for You  I like eat delicious food. That's I'm cooking ...   \n",
       "1  10 Best Foods for You    This help eating healthy exercise regular basis   \n",
       "3  10 Best Foods for You         Works great especially going grocery store   \n",
       "4  10 Best Foods for You                                       Best idea us   \n",
       "5  10 Best Foods for You                                           Best way   \n",
       "\n",
       "   sentences_count  characters_count  spaces_count  count_words  \\\n",
       "0              2.0             122.0          20.0         22.0   \n",
       "1              1.0              47.0           6.0          7.0   \n",
       "3              1.0              42.0           5.0          6.0   \n",
       "4              1.0              12.0           2.0          3.0   \n",
       "5              1.0               8.0           1.0          2.0   \n",
       "\n",
       "   duplicates_count  chars_excl_spaces_count  emoji_count  \\\n",
       "0               6.0                    102.0          0.0   \n",
       "1               0.0                     41.0          0.0   \n",
       "3               0.0                     37.0          0.0   \n",
       "4               0.0                     10.0          0.0   \n",
       "5               0.0                      7.0          0.0   \n",
       "\n",
       "   whole_numbers_count  ...  spelling_quality  spelling_quality_summarised  \\\n",
       "0                  1.0  ...               Bad                          Bad   \n",
       "1                  0.0  ...         Very good                         Good   \n",
       "3                  0.0  ...         Very good                         Good   \n",
       "4                  0.0  ...         Very good                         Good   \n",
       "5                  0.0  ...         Very good                         Good   \n",
       "\n",
       "   ease_of_reading_score  ease_of_reading_quality  ease_of_reading_summarised  \\\n",
       "0                  86.20                     Easy                        Easy   \n",
       "1                  38.99                Difficult                   Difficult   \n",
       "3                  48.47                Difficult                   Difficult   \n",
       "4                 119.19                Very Easy                        Easy   \n",
       "5                 120.21                Very Easy                        Easy   \n",
       "\n",
       "   grammar_check_score  grammar_check original_Sentiment  \\\n",
       "0                  5.0       5 issues           Positive   \n",
       "1                  0.0      No issues           Positive   \n",
       "3                  0.0      No issues           Positive   \n",
       "4                  1.0        1 issue           Positive   \n",
       "5                  0.0      No issues           Positive   \n",
       "\n",
       "  original_Sentiment_Polarity  original_Sentiment_Subjectivity  \n",
       "0                        1.00                         0.533333  \n",
       "1                        0.25                         0.288462  \n",
       "3                        0.40                         0.875000  \n",
       "4                        1.00                         0.300000  \n",
       "5                        1.00                         0.300000  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preparation\n",
    "csv_data = pd.read_csv('extended_googleplaystore_user_reviews.csv')\n",
    "csv_data = csv_data[csv_data['Translated_Review'].notna()]\n",
    "p=csv_data['original_Sentiment_Polarity'].max()\n",
    "q=csv_data['original_Sentiment_Polarity'].min()\n",
    "text_with_max_length = csv_data['Translated_Review'][csv_data['Translated_Review'].str.len().idxmax()]\n",
    "\n",
    "print(p,q,len(csv_data))\n",
    "print('MAXIMUN LENGTH REVIEW---->',text_with_max_length)\n",
    "csv_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69cdcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentSatisfaction:\n",
    "    def __init__(self,satisfaction_index):\n",
    "        self.satisfaction_index = satisfaction_index\n",
    "    def get_sentiment_satisfaction(self):\n",
    "        if self.satisfaction_index <= -0.6:\n",
    "            return \"very_negative\"\n",
    "        elif self.satisfaction_index <= -0.2:\n",
    "            return \"negetive\"\n",
    "        elif self.satisfaction_index <= 0.2:\n",
    "            return \"neutral\"\n",
    "        elif self.satisfaction_index <= 0.6:\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"very_positive\"\n",
    "\n",
    "satisfaction_class = [\"very_negative\",\"negative\",\"neutral\",\"positive\",\"very_positive\"]\n",
    "csv_data['result'] = csv_data['original_Sentiment_Polarity'].apply(lambda x:SentimentSatisfaction(x).get_sentiment_satisfaction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068a1476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>result</th>\n",
       "      <th>original_Sentiment_Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I like eat delicious food. That's I'm cooking ...</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This help eating healthy exercise regular basis</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Works great especially going grocery store</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Best idea us</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Best way</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Translated_Review         result  \\\n",
       "0  I like eat delicious food. That's I'm cooking ...  very_positive   \n",
       "1    This help eating healthy exercise regular basis       positive   \n",
       "3         Works great especially going grocery store       positive   \n",
       "4                                       Best idea us  very_positive   \n",
       "5                                           Best way  very_positive   \n",
       "\n",
       "   original_Sentiment_Polarity  \n",
       "0                         1.00  \n",
       "1                         0.25  \n",
       "3                         0.40  \n",
       "4                         1.00  \n",
       "5                         1.00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = csv_data.copy()\n",
    "data = csv_data[['Translated_Review','result','original_Sentiment_Polarity']]\n",
    "# Shuffle Row Order\n",
    "# data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d78a0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rujal/notebook/jupyterenv/lib/python3.8/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>result</th>\n",
       "      <th>original_Sentiment_Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like eat delicious food cooking food case best...</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>help eating healthy exercise regular basis</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>work great especially going grocery store</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best idea u</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>best way</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Translated_Review         result  \\\n",
       "0  like eat delicious food cooking food case best...  very_positive   \n",
       "1         help eating healthy exercise regular basis       positive   \n",
       "3          work great especially going grocery store       positive   \n",
       "4                                        best idea u  very_positive   \n",
       "5                                           best way  very_positive   \n",
       "\n",
       "   original_Sentiment_Polarity  \n",
       "0                         1.00  \n",
       "1                         0.25  \n",
       "3                         0.40  \n",
       "4                         1.00  \n",
       "5                         1.00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------\n",
    "# Cleaning and Wrangling of Data\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Remove URLs and mentions from text\n",
    "data.loc[:,'Translated_Review'] = data['Translated_Review'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "data.loc[:,'Translated_Review'] = data['Translated_Review'].apply(lambda x: re.sub(r'@\\S+', '', x))\n",
    "\n",
    "\n",
    "\n",
    "# Remove non-alphabetic characters and convert to lowercase\n",
    "data.loc[:,'Translated_Review'] = data['Translated_Review'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x.lower()))\n",
    "\n",
    "# Tokenize text\n",
    "data.loc[:,'Translated_Review'] = data['Translated_Review'].apply(lambda x: nltk.word_tokenize(x))\n",
    "data.loc[:,'Translated_Review'] = data['Translated_Review'].apply(lambda x: [value for value in x if not re.match(r'^-?\\d+\\.?\\d*$', value)])\n",
    "\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "data.loc[:,'Translated_Review'] = data['Translated_Review'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Lemmatize text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data.loc[:,'Translated_Review'] = data['Translated_Review'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Join tokens back into strings\n",
    "data.loc[:,'Translated_Review'] = data['Translated_Review'].apply(lambda x: ' '.join(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44255d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Machine Learning\n",
    "m_data = data.copy()\n",
    "review_data = m_data['Translated_Review']\n",
    "result_data = m_data['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91361c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------\n",
    "# Preprocessing of Data \n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the data using the vectorizer object\n",
    "c_vectorized_data = vectorizer.fit_transform(review_data)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(c_vectorized_data, result_data, test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "474b230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tf-IDF Vectorizer object\n",
    "t_vectorized_data = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "t_vectorized_data.fit(review_data)\n",
    "\n",
    "# Fit and transform the data using the vectorizer object\n",
    "t_vectorized_data = vectorizer.transform(review_data)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(t_vectorized_data, result_data, test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45ef18fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rujal/notebook/jupyterenv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/rujal/notebook/jupyterenv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/rujal/notebook/jupyterenv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/rujal/notebook/jupyterenv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/rujal/notebook/jupyterenv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8402351055303233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rujal/notebook/jupyterenv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression and Count Vectorizer\n",
    "# Train a machine learning model on the training set\n",
    "l_model = LogisticRegression()\n",
    "\n",
    "# For HyperParameter Tuning\n",
    "param_grid={\n",
    "    'warm_start': [True], \n",
    "    'solver': ['sag'], \n",
    "    'penalty': ['l2'], \n",
    "    'max_iter': [200], \n",
    "    'C': [206.913808111479]\n",
    "}\n",
    "\n",
    "# Perform random search with cross-validation\n",
    "random_search = GridSearchCV(l_model, param_grid=param_grid, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Use the best model to make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3e9d8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8088431739246593\n"
     ]
    }
   ],
   "source": [
    "# For TFIDF and Support Vector Machine\n",
    "# Train a machine learning model on the training set\n",
    "t_l_model = SVC()\n",
    "\n",
    "t_l_model.fit(X_train, y_train)\n",
    "y_pred = t_l_model.predict(X_test)\n",
    "\n",
    "# Use the best model to make predictions\n",
    "y_pred = t_l_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80127478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is maximum length review 2241\n",
      "Padded Data:  [[   4  701 3142 ...    0    0    0]\n",
      " [  61 1223 1186 ...    0    0    0]\n",
      " [  11    8  344 ...    0    0    0]\n",
      " ...\n",
      " [1092    7  358 ...    0    0    0]\n",
      " [1413  477   82 ...    0    0    0]\n",
      " [ 200    7 2132 ...    0    0    0]]\n",
      "Shape of data tensor:  (37427, 100)\n",
      "Shape of result tensor:  (37427,)\n"
     ]
    }
   ],
   "source": [
    "# For Deep Learning\n",
    "d_data = data.copy()\n",
    "d_data = d_data[['Translated_Review','result']]\n",
    "texts = d_data['Translated_Review']\n",
    "max_length = texts.str.len().max()\n",
    "print('This is maximum length review',max_length)\n",
    "\n",
    "# consider only the top 10000 words\n",
    "max_words = 20000 \n",
    "\n",
    "# Tokenize and encode\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=10000,  # maximum vocabulary size\n",
    "    output_mode='int',  # output integer-encoded sequences\n",
    "    output_sequence_length=100  # fixed sequence length\n",
    ")\n",
    "vectorizer.adapt(texts)\n",
    "encoded_reviews = vectorizer(texts)\n",
    "\n",
    "def getUniqueValue(x):\n",
    "    if x=='very_positive':\n",
    "        return 5\n",
    "    if x=='positive':\n",
    "        return 4\n",
    "    if x=='neutral':\n",
    "        return 3\n",
    "    if x=='negative':\n",
    "        return 2\n",
    "    if x=='very_negative':\n",
    "        return 1\n",
    "\n",
    "# 2241 (is maximum length review)\n",
    "# result = np.asarray(d_data['result'])\n",
    "result = d_data['result'].apply(lambda x:getUniqueValue(x))\n",
    "tensor_data =  encoded_reviews.numpy()\n",
    "print(\"Padded Data: \", tensor_data)\n",
    "print(\"Shape of data tensor: \", encoded_reviews.shape)\n",
    "print(\"Shape of result tensor: \", result.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b57952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train .....\n",
      "Epoch 1/10\n",
      "547/749 [====================>.........] - ETA: 25s - loss: nan - accuracy: 5.7130e-05"
     ]
    }
   ],
   "source": [
    "max_words = 20000 \n",
    "max_length = 100\n",
    "\n",
    "b_model = Sequential()\n",
    "b_model.add(Embedding(max_words, 128, input_length=max_length))\n",
    "b_model.add(Bidirectional(LSTM(64)))\n",
    "b_model.add(Dropout(0.5))\n",
    "b_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Split padded data into Training and Testing Data\n",
    "x_train, x_test, Y_train, Y_test = train_test_split(tensor_data, result, test_size=0.2,random_state=42)\n",
    "# Split the training data further into training and validation sets\n",
    "x_train, x_val, Y_train, y_val = train_test_split(x_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# x_train, Y_train will be used for training the BiLSTM model\n",
    "# x_val, y_val will be used for validation during training\n",
    "# x_test, Y_test will be used for final evaluation of the trained model\n",
    "\n",
    "b_model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "batch_size = 128\n",
    "print('Train .....')\n",
    "b_model.fit(x_train, Y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Polarity Score Estimation\n",
    "p_data = data.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
